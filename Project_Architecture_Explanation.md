# Enhanced RAG Chatbot: Complete Architecture Explanation

## ğŸ“‹ Table of Contents

1. [Data Flow Design](#data-flow-design)
2. [Source Selection: APIs Implementation](#source-selection-apis-implementation)
3. [Local RAG: Indexing, Data Cleaning, and Chunking](#local-rag-indexing-data-cleaning-and-chunking)
4. [Filtering & Ranking: Relevance Improvement](#filtering--ranking-relevance-improvement)
5. [Multimodal Processing](#multimodal-processing)

---

## ğŸ”„ Data Flow Design

### **Complete System Flow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INPUT (Streamlit UI)                     â”‚
â”‚              "What are the departure times for Bus 91M?"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Query Preprocessing                                    â”‚
â”‚  - Check if image query (get_uploaded_image_filename)          â”‚
â”‚  - Check if self-contained query (is_self_contained_query)     â”‚
â”‚  - Check if factual query (is_factual_query)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Query Classification (QueryClassifier)                â”‚
â”‚  Location: query_classifier.py                                  â”‚
â”‚                                                                 â”‚
â”‚  Uses GPT-4o to classify query into:                            â”‚
â”‚  - local_rag: Questions about uploaded documents               â”‚
â”‚  - web_search: Real-time data, current events                  â”‚
â”‚  - hybrid: Both local docs + web info needed                   â”‚
â”‚                                                                 â”‚
â”‚  Classification Logic:                                         â”‚
â”‚  1. LLM-based classification (primary)                          â”‚
â”‚  2. Fallback keyword-based classification                      â”‚
â”‚  3. Real-time query detection                                  â”‚
â”‚  4. Document availability check                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                           â”‚
        â†“                                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOCAL RAG PATH   â”‚                    â”‚  WEB SEARCH PATH  â”‚
â”‚                   â”‚                    â”‚                   â”‚
â”‚  Step 3a:         â”‚                    â”‚  Step 3b:         â”‚
â”‚  FAISS Retrieval  â”‚                    â”‚  Source Selection â”‚
â”‚  - Vector search  â”‚                    â”‚  - OpenAI Web     â”‚
â”‚  - Top 10 docs    â”‚                    â”‚  - Tavily         â”‚
â”‚                   â”‚                    â”‚  - Wikipedia      â”‚
â”‚  Step 4a:         â”‚                    â”‚  - ArXiv          â”‚
â”‚  Reranking       â”‚                    â”‚  - Google API      â”‚
â”‚  - Cross-Encoder â”‚                    â”‚  - Bing API       â”‚
â”‚  - BM25 fallback â”‚                    â”‚                   â”‚
â”‚  - Top 4 docs     â”‚                    â”‚  Step 4b:         â”‚
â”‚                   â”‚                    â”‚  Result Merging  â”‚
â”‚  Step 5a:         â”‚                    â”‚  - Combine OpenAI â”‚
â”‚  Context Build   â”‚                    â”‚    + Tavily       â”‚
â”‚  - Format docs    â”‚                    â”‚  - Remove dupes    â”‚
â”‚  - Add metadata   â”‚                    â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6: Context Assembly                                       â”‚
â”‚  - Combine retrieved documents/web results                      â”‚
â”‚  - Add conversation history (last 6 messages)                  â”‚
â”‚  - Format for LLM                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 7: LLM Response Generation                                â”‚
â”‚  Location: generate_response_with_context()                     â”‚
â”‚                                                                 â”‚
â”‚  - Model: GPT-4o                                                â”‚
â”‚  - System prompt based on datasource                            â”‚
â”‚  - Includes conversation history                                â”‚
â”‚  - Date-aware instructions (for "next" queries)                â”‚
â”‚  - Multi-part query handling                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 8: Response Display                                       â”‚
â”‚  - Show answer                                                  â”‚
â”‚  - Display sources (ğŸ“š X Sources Used)                          â”‚
â”‚  - Show performance metrics (â±ï¸ Performance Metrics)            â”‚
â”‚  - Show routing details (ğŸ” Source Information)                â”‚
â”‚  - Save to chat history                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Key Components in Data Flow**

1. **QueryClassifier** (`query_classifier.py`)

   - Analyzes query intent
   - Determines best data source
   - Returns routing decision with confidence

2. **IntelligentSourceRouter** (`intelligent_source_router.py`)

   - Orchestrates the routing process
   - Executes retrieval based on classification
   - Applies reranking for local RAG
   - Formats context for LLM

3. **WebSearchEnhanced** (`web_search_tavily.py`)

   - Manages multiple search APIs
   - Implements fallback chain
   - Combines results from multiple sources

4. **Response Generator** (`enhanced_rag_chatbot.py`)
   - Generates final answer using GPT-4o
   - Incorporates conversation history
   - Handles different query types

---

## ğŸŒ Source Selection: APIs Implementation

### **Architecture Overview**

The system uses a **multi-tier fallback chain** to ensure reliable search results:

```
Priority Order:
1. OpenAI Web Search (for real-time queries)
2. Tavily API (AI-powered search)
3. Wikipedia API
4. ArXiv API (academic papers)
5. Google Custom Search API
6. Bing Search API
7. Mock Results (guaranteed fallback)
```

### **Implementation Details**

#### **1. OpenAI Web Search Integration**

**File:** `web_search_openai.py`

**Purpose:** Handle real-time queries requiring current data

**Features:**

- **Automatic Detection:** Detects real-time queries (bus schedules, stock prices, weather)
- **Three Modes:**
  - Fast search (`reasoning_level="low"`): Quick lookups
  - Thorough search (`reasoning_level="medium"`): Complex queries
  - Deep research (`reasoning_level="high"`): Extended investigations
- **Dual API Support:**
  - Responses API (primary)
  - Chat Completions API (fallback with search models)

**When Used:**

- Real-time queries detected (bus schedules, stock prices, weather, events)
- Automatically triggered in `web_search_tavily.py` line 489

#### **2. Tavily API (Primary Web Search)**

**File:** `web_search_tavily.py` lines 24-125

**Purpose:** AI-powered search optimized for LLM context

**Features:**

- Direct answer extraction
- Raw content inclusion
- High-quality results for general queries

#### **3. Wikipedia API**

**File:** `web_search_tavily.py` lines 255-297

**Purpose:** Fallback for general knowledge queries

**Features:**

- Free, reliable source
- Good for definitions and explanations
- No API key required

#### **4. ArXiv API**

**File:** `web_search_tavily.py` lines 299-347

**Purpose:** Academic paper search

**Features:**

- Sorted by submission date
- Good for research queries
- XML-based API

#### **5. Google & Bing APIs**

**File:** `web_search_tavily.py` lines 349-437

**Purpose:** Commercial search engines (if API keys configured)

**Features:**

- High-quality results
- Requires API keys
- Used as fallback

### **Hybrid Search Strategy**

**Key Innovation:** Combines results from multiple sources

**Implementation:** `web_search_tavily.py` lines 484-524

**Process:**

1. Collects results from OpenAI Web Search first (if real-time query detected)
2. Also searches with Tavily (even if OpenAI succeeded)
3. Combines results from both sources
4. Removes duplicates by URL (tracks existing URLs to avoid duplicates)
5. Returns combined results (limited to max_results)

**Benefits:**

- More comprehensive coverage
- Redundancy if one source fails
- Better source diversity

### **Real-Time Query Detection**

**File:** `web_search_tavily.py` lines 206-224

**Keywords Detected:**

- Transportation: "departure time", "bus schedule", "train schedule"
- Financial: "stock price", "stock performance", "nvidia", "amd"
- Weather: "weather", "temperature", "forecast"
- Time-based: "today", "now", "current", "latest", "recent"

**Detection Method:**

- Checks if any real-time keywords appear in the query (case-insensitive)
- Returns True if match found, enabling OpenAI Web Search priority routing

---

## ğŸ“š Local RAG: Indexing, Data Cleaning, and Chunking

### **Complete Pipeline**

```
Uploaded Files (PDF/Images)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Data Extraction             â”‚
â”‚  - PDF: PyPDF2 text extraction       â”‚
â”‚  - Images: EasyOCR + GPT Vision      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Data Cleaning                â”‚
â”‚  - Error handling                     â”‚
â”‚  - Format normalization              â”‚
â”‚  - Metadata extraction                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Chunking                     â”‚
â”‚  - RecursiveCharacterTextSplitter     â”‚
â”‚  - Chunk size: 1000 chars             â”‚
â”‚  - Overlap: 200 chars                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Embedding                    â”‚
â”‚  - OpenAI Embeddings (text-embedding) â”‚
â”‚  - Vector representation              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Indexing                     â”‚
â”‚  - FAISS Vector Store                 â”‚
â”‚  - Metadata storage                   â”‚
â”‚  - Persistent storage                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **1. Data Extraction**

**File:** `enhanced_rag_chatbot.py` lines 57-181

#### **PDF Processing** (lines 57-70)

**Process:**

- Uses PyPDF2 to read PDF files
- Extracts text page by page
- Adds page number markers for reference
- Handles extraction errors gracefully

**Features:**

- Page-by-page extraction
- Page number tracking
- Error handling

#### **Image Processing** (lines 73-181)

**Dual Processing Strategy:**

1. **EasyOCR** (lines 92-101)

   - Text extraction from images
   - Handles text-heavy images
   - Fast processing

2. **GPT Vision API** (lines 103-155)
   - Visual analysis
   - Object/scene recognition
   - Contextual understanding
   - University/landmark identification

**Combined Output:**

- If both OCR and Vision succeed: Combines both with clear section markers
- If only OCR succeeds: Uses OCR text only
- If only Vision succeeds: Uses Vision description only
- Result: Comprehensive image representation for search and retrieval

### **2. Data Cleaning**

**Location:** `enhanced_rag_chatbot.py` lines 184-222

**Process:**

- File type validation
- Duplicate file detection
- Error handling for extraction failures
- Metadata preservation

**Process:**

- Validates file types (PDF, PNG, JPG, JPEG, GIF, BMP)
- Skips already processed files to avoid duplicates
- Extracts text based on file type
- Stores extracted content with source filename as metadata
- Handles errors gracefully without crashing the system

### **3. Chunking Strategy**

**File:** `enhanced_rag_chatbot.py` lines 227-244

**Algorithm:** RecursiveCharacterTextSplitter (LangChain)

**Parameters:**

- **Chunk Size:** 1000 characters
- **Chunk Overlap:** 200 characters
- **Length Function:** Character count

**Process:**

- Uses RecursiveCharacterTextSplitter from LangChain
- Splits text into chunks of 1000 characters
- Each chunk overlaps with previous chunk by 200 characters
- Preserves metadata (source file, chunk index) for each chunk
- Handles various text structures (paragraphs, sentences, etc.)
- Recursive splitting ensures chunks respect natural text boundaries

**Why This Strategy:**

- **1000 chars:** Good balance between context and granularity
- **200 overlap:** Prevents information loss at chunk boundaries
- **Recursive splitting:** Handles various text structures (paragraphs, sentences, etc.)

### **4. Embedding & Indexing**

**File:** `enhanced_rag_chatbot.py` lines 246-272

**Embedding Model:** OpenAI Embeddings (`text-embedding-ada-002` or similar)

**Vector Store:** FAISS (Facebook AI Similarity Search)

**Process:**

- Creates OpenAI embeddings for each text chunk using OpenAI Embeddings API
- Converts text chunks into high-dimensional vectors (typically 1536 dimensions)
- Stores vectors in FAISS index for fast similarity search
- Preserves metadata (source file, chunk index) with each vector
- Supports incremental updates (can add new documents to existing store)
- Enables fast similarity search using cosine similarity

**Features:**

- **Fast similarity search:** FAISS enables efficient vector search
- **Metadata tracking:** Each chunk tracks source file and chunk index
- **Incremental updates:** Can add new documents to existing store

**Retrieval Process:**

- Creates a retriever from the FAISS vector store
- Embeds the user query using the same embedding model
- Performs vector similarity search (cosine similarity) in FAISS
- Retrieves top 10 most similar documents based on query embedding
- Returns documents with their metadata (source file, chunk index) for context assembly

---

## ğŸ¯ Filtering & Ranking: Relevance Improvement

### **Two-Stage Retrieval Architecture**

```
Stage 1: Initial Retrieval (FAISS)
â”œâ”€ Vector similarity search
â”œâ”€ Retrieves top 10 candidates
â””â”€ Fast but may miss semantic nuances

Stage 2: Reranking
â”œâ”€ Cross-Encoder (primary)
â”œâ”€ BM25 (fallback)
â””â”€ Returns top 4 most relevant
```

### **Implementation**

**File:** `intelligent_source_router.py` lines 148-235

#### **1. Reranking Function**

**Location:** `intelligent_source_router.py` lines 148-170

**Process:**

- Checks if reranking is needed (requires more than 1 document)
- Tries Cross-Encoder reranking first (primary method)
- Falls back to BM25 if Cross-Encoder fails
- Returns top K most relevant documents

#### **2. Cross-Encoder Reranking (Primary)**

**Location:** `intelligent_source_router.py` lines 172-199

**Model:** `cross-encoder/ms-marco-MiniLM-L-6-v2`

**How It Works:**

1. Creates query-document pairs (one pair per document)
2. Cross-encoder processes each pair together (full attention)
3. Computes relevance scores for each pair
4. Sorts documents by score (highest first)
5. Returns top K most relevant documents

**Model Details:**

- Model: `cross-encoder/ms-marco-MiniLM-L-6-v2`
- Lazy loading: Model loaded only when first needed
- Pre-trained on MS MARCO dataset (industry standard for IR)

**Advantages:**

- **Full attention:** Query and document processed together
- **Contextual understanding:** Understands query-document relationships
- **High accuracy:** Trained on MS MARCO dataset

#### **3. BM25 Reranking (Fallback)**

**Location:** `intelligent_source_router.py` lines 201-235

**Algorithm:** Best Matching 25 (classic IR algorithm)

**How It Works:**

1. Tokenizes query and documents into words
2. Calculates BM25 scores (TF-IDF variant) for each document
3. Scores based on term frequency and inverse document frequency
4. Sorts documents by BM25 score
5. Returns top K documents

**Algorithm Details:**

- Classic information retrieval algorithm
- Based on term frequency (TF) and inverse document frequency (IDF)
- No model dependencies (pure statistical approach)
- Fast and lightweight

**Advantages:**

- Fast and lightweight
- No model dependencies
- Good for keyword-based relevance

### **When Reranking is Applied**

**Location:** `intelligent_source_router.py` lines 358-364, 425-429

**Application Points:**

- Applied in `_retrieve_local()` when more than 1 document is retrieved
- Applied in `_retrieve_hybrid()` for local documents when more than 1 document is retrieved
- Only reranks if multiple documents are available (no need for single document)

**Conditions:**

- âœ… Applied to local RAG retrieval
- âœ… Applied to hybrid retrieval (local documents)
- âŒ Not applied to web search results (future enhancement)

### **Performance Impact**

**Before Reranking:**

- Documents ordered by embedding similarity
- May miss semantic nuances
- Example: "sculpture's symbolic meaning" might return general "sculpture" docs first

**After Reranking:**

- Documents ordered by query-document relevance
- Most relevant documents prioritized
- Example: Documents about "symbolic meaning" ranked higher

**Trade-offs:**

- **Accuracy:** âœ… Significantly improved
- **Speed:** âš ï¸ Adds ~0.1-0.5s per query
- **Cost:** âœ… Free (uses pre-trained models)

---

## ğŸ–¼ï¸ Multimodal Processing

### **Supported File Types**

1. **PDF Documents** - Text extraction
2. **Images** - OCR + Vision analysis

### **Image Processing Pipeline**

**File:** `enhanced_rag_chatbot.py` lines 73-181

#### **Step 1: EasyOCR Text Extraction**

**Purpose:** Extract visible text from images

**Process:**

- Initializes EasyOCR reader for English text
- Processes image to detect and extract text
- Returns extracted text as a string
- Handles text-heavy images efficiently

**Use Cases:**

- Text-heavy images (documents, signs, labels)
- Quick text extraction

#### **Step 2: GPT Vision API Analysis**

**Purpose:** Visual understanding and context

**Process:**

- Converts image to base64 format for API transmission
- Creates detailed prompt requesting comprehensive image analysis
- Sends image and prompt to GPT-4o Vision API
- Receives detailed description including objects, scenes, locations, landmarks, and symbolic meanings

**Prompt Requests:**

- Objects, people, buildings, or scenes visible
- Setting, location, or environment
- Text visible in the image
- Colors, composition, and visual elements
- Notable features or characteristics
- Specific landmarks, sculptures, buildings, or locations (with names)
- University identification if applicable
- Symbolic meaning for sculptures or monuments

**Features:**

- Object/scene recognition
- Location identification
- University/landmark detection
- Symbolic meaning analysis
- Contextual understanding

#### **Step 3: Intelligent Combination**

**Location:** `enhanced_rag_chatbot.py` lines 157-176

**Process:**

- If both OCR and Vision succeed: Combines both with clear section markers
- If only OCR succeeds: Uses OCR text only
- If only Vision succeeds: Uses Vision description only
- Result: Comprehensive image representation for search and retrieval

### **Direct Image Question Answering**

**File:** `enhanced_rag_chatbot.py` lines 382-433

**Purpose:** Answer questions about uploaded images directly

**How It Works:**

1. Detects if query is about an uploaded image
2. Retrieves image bytes from session state
3. Converts image to base64 format for API transmission
4. Sends image + question directly to GPT Vision API
5. Returns answer without RAG processing

**Benefits:**

- More accurate than using stored descriptions
- Real-time image analysis
- Better for specific questions about image content

### **Image Query Detection**

**File:** `enhanced_rag_chatbot.py` lines 436-491

**Smart Detection:**

- **Explicit keywords:** "image", "picture", "photo", "photograph", "img"
- **Content queries:** "identify", "what is this", "describe", "explain", "where is", "symbolic", "meaning", "significance"
- **Context-aware:** Detects queries about image content even without "image" keyword
- **Single image handling:** If only one image uploaded, automatically uses it for content queries
- **Multiple image handling:** Matches by filename if mentioned in query

---

## ğŸ”— Integration Points

### **How Components Work Together**

1. **QueryClassifier** â†’ Analyzes query â†’ Routes to appropriate source
2. **IntelligentSourceRouter** â†’ Executes retrieval â†’ Applies reranking
3. **WebSearchEnhanced** â†’ Searches multiple APIs â†’ Combines results
4. **Local RAG** â†’ FAISS retrieval â†’ Reranking â†’ Context formatting
5. **Response Generator** â†’ Combines context + history â†’ Generates answer

### **Key Design Decisions**

1. **Hybrid Search:** Combines OpenAI + Tavily for better coverage
2. **Two-Stage Retrieval:** Fast initial search + accurate reranking
3. **Dual Image Processing:** OCR + Vision for comprehensive understanding
4. **Conversation Memory:** Last 6 messages for context
5. **Date-Aware Queries:** Special handling for "next"/"earliest" queries

---

## ğŸ“Š Performance Characteristics

### **Timing Breakdown**

- **Query Classification:** ~1-2 seconds (LLM call)
- **Search/Routing:** ~12-21 seconds (multiple API calls)
- **Reranking:** ~0.1-0.5 seconds (Cross-Encoder)
- **LLM Generation:** ~2-3 seconds (GPT-4o)
- **Total:** ~15-24 seconds

### **Optimization Strategies**

1. **Disabled full content fetching:** Faster web search (snippets only)
2. **Lazy model loading:** Cross-Encoder loaded only when needed
3. **Parallel search:** OpenAI + Tavily searched simultaneously
4. **Caching:** Vector store persists across sessions

---

## ğŸ¯ Summary

**This system implements:**

âœ… **Intelligent Source Routing** - Automatically selects best data source  
âœ… **Multi-Source Search** - Combines OpenAI + Tavily + fallbacks  
âœ… **Two-Stage Retrieval** - Fast initial search + accurate reranking  
âœ… **Multimodal Processing** - PDF + Images (OCR + Vision)  
âœ… **Conversation Memory** - Maintains context across messages  
âœ… **Performance Monitoring** - Tracks timing for all operations

**Result:** A production-ready RAG system that can handle diverse queries with high accuracy and reliability.
