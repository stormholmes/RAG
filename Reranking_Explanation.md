# Filtering & Ranking: How We Improved Relevance

## ğŸ“‹ Overview

Our system uses a **two-stage retrieval approach** to improve relevance:

1. **Initial Retrieval**: FAISS similarity search retrieves candidate documents
2. **Reranking**: Advanced algorithms reorder documents by query-document relevance

This significantly improves answer quality by ensuring the most relevant documents are prioritized.

---

## ğŸ” The Problem: Why Reranking is Needed

### **Initial Retrieval Limitations:**

**FAISS Vector Search** (Initial Stage):

- Uses **cosine similarity** between query and document embeddings
- Fast but sometimes misses semantic nuances
- May return documents that are "similar" but not directly relevant to the specific query
- Example: Query "What is the sculpture's symbolic meaning?" might retrieve documents about "sculpture" but not specifically about "symbolic meaning"

### **Solution: Reranking**

Reranking uses **query-document interaction models** that:

- Understand the **specific relationship** between query and each document
- Score documents based on **relevance to the exact query**
- Reorder results to put most relevant documents first

---

## ğŸ¯ Two-Stage Retrieval Process

### **Stage 1: Initial Retrieval (FAISS)**

```python
# From enhanced_rag_chatbot.py line 782
retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 10})
docs = local_retriever.get_relevant_documents(query)
```

**What happens:**

- Retrieves **top 10 documents** using FAISS vector similarity
- Fast but may not be perfectly ordered by relevance
- Returns candidate documents for reranking

### **Stage 2: Reranking (Cross-Encoder/BM25)**

```python
# From intelligent_source_router.py lines 358-364
if len(docs) > 1:
    docs = self._rerank_documents(query, docs, top_k=4)
    # Returns top 4 most relevant documents
```

**What happens:**

- Takes the 10 candidate documents
- Scores each document against the query
- Reorders by relevance score
- Returns **top 4 most relevant documents**

**Result:** The final 4 documents sent to the LLM are the most relevant ones.

---

## ğŸ§  Reranking Algorithm #1: Cross-Encoder (Primary)

### **How It Works:**

**Model:** `cross-encoder/ms-marco-MiniLM-L-6-v2`

- Pre-trained on MS MARCO dataset (Microsoft's large-scale information retrieval dataset)
- Specifically designed for query-document relevance scoring

**Process:**

1. **Input Preparation:**

   ```python
   # Creates query-document pairs
   pairs = [(query, doc.page_content) for doc in documents]
   # Example: [("What is the sculpture?", "The sculpture is a red abstract piece..."), ...]
   ```

2. **Relevance Scoring:**

   ```python
   # Cross-encoder processes each pair together
   scores = self.cross_encoder.predict(pairs)
   # Returns: [0.85, 0.72, 0.91, 0.68, ...] (relevance scores)
   ```

3. **Reordering:**
   ```python
   # Sort by score (highest first)
   scored_docs.sort(key=lambda x: x[0], reverse=True)
   # Select top 4
   reranked_docs = [doc for score, doc in scored_docs[:top_k]]
   ```

### **Why Cross-Encoder is Better:**

**Cross-Encoder vs. Bi-Encoder (FAISS):**

| Aspect          | Bi-Encoder (FAISS)                    | Cross-Encoder (Reranking)                 |
| --------------- | ------------------------------------- | ----------------------------------------- |
| **Processing**  | Query and document encoded separately | Query and document processed together     |
| **Interaction** | No direct interaction                 | Full attention between query and document |
| **Accuracy**    | Good for initial retrieval            | Better for fine-grained relevance         |
| **Speed**       | Very fast (pre-computed)              | Slower but more accurate                  |
| **Use Case**    | First-stage retrieval                 | Second-stage reranking                    |

**Key Advantage:** Cross-encoder sees the query and document **together**, allowing it to understand specific relationships like:

- "sculpture" + "symbolic meaning" â†’ Higher score for documents discussing symbolism
- "sculpture" + "location" â†’ Higher score for documents mentioning locations

---

## ğŸ”¢ Reranking Algorithm #2: BM25 (Fallback)

### **When Used:**

- If Cross-Encoder fails (import error, model loading issue)
- As a reliable fallback algorithm

### **How It Works:**

**BM25 (Best Matching 25):**

- Classic information retrieval algorithm
- Based on **term frequency** and **inverse document frequency** (TF-IDF variant)
- Scores documents based on how many query terms appear in the document

**Process:**

1. **Tokenization:**

   ```python
   # Split documents and query into tokens
   tokens = doc.page_content.lower().split()
   query_tokens = query.lower().split()
   ```

2. **BM25 Scoring:**

   ```python
   # Calculate BM25 scores
   bm25 = BM25Okapi(tokenized_docs)
   doc_scores = bm25.get_scores(query_tokens)
   # Returns scores based on term frequency
   ```

3. **Reordering:**
   ```python
   # Sort by BM25 score
   scored_docs.sort(key=lambda x: x[0], reverse=True)
   ```

### **BM25 Advantages:**

- Fast and lightweight
- No model dependencies
- Good for keyword-based relevance
- Reliable fallback

---

## ğŸ“Š Complete Reranking Flow

```
User Query: "What is the sculpture's symbolic meaning?"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: Initial Retrieval (FAISS)                      â”‚
â”‚ - Vector similarity search                              â”‚
â”‚ - Retrieves top 10 candidate documents                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: Reranking                                      â”‚
â”‚                                                          â”‚
â”‚ Try Cross-Encoder:                                      â”‚
â”‚   â”œâ”€ Create (query, doc) pairs                         â”‚
â”‚   â”œâ”€ Score each pair: [0.91, 0.72, 0.85, 0.68, ...]   â”‚
â”‚   â””â”€ Reorder by score                                   â”‚
â”‚                                                          â”‚
â”‚ If fails â†’ Use BM25:                                    â”‚
â”‚   â”œâ”€ Tokenize query and documents                      â”‚
â”‚   â”œâ”€ Calculate BM25 scores                              â”‚
â”‚   â””â”€ Reorder by score                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Final Selection                                          â”‚
â”‚ - Top 4 most relevant documents                         â”‚
â”‚ - Sent to LLM for answer generation                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ How This Improves Relevance

### **Example Scenario:**

**Query:** "What is the sculpture's symbolic meaning?"

**Initial FAISS Retrieval (top 10):**

1. Document about sculpture's physical description (similarity: 0.85)
2. Document about sculpture's location (similarity: 0.82)
3. Document about sculpture's symbolic meaning (similarity: 0.80) â† **Actually most relevant!**
4. Document about other sculptures (similarity: 0.78)
5. ...

**After Cross-Encoder Reranking (top 4):**

1. Document about sculpture's symbolic meaning (score: 0.91) â† **Now first!**
2. Document about sculpture's physical description (score: 0.72)
3. Document about sculpture's location (score: 0.68)
4. Document about sculpture's history (score: 0.65)

**Result:** The LLM receives the most relevant document first, leading to better answers.

---

## ğŸ”§ Implementation Details

### **Code Location:**

- **Main Function**: `intelligent_source_router.py` lines 148-235
- **Applied In**: `_retrieve_local()` (line 361) and `_retrieve_hybrid()` (line 427)

### **Key Parameters:**

- **Initial Retrieval**: `k=10` (retrieve 10 candidates)
- **Reranking Output**: `top_k=4` (return top 4 most relevant)
- **Model**: `cross-encoder/ms-marco-MiniLM-L-6-v2`

### **When Reranking is Applied:**

- âœ… **Local RAG retrieval** - Always applied when >1 document retrieved
- âœ… **Hybrid retrieval** - Applied to local documents in hybrid mode
- âŒ **Web search** - Not currently applied (could be future improvement)

### **Error Handling:**

- If Cross-Encoder fails â†’ Falls back to BM25
- If BM25 fails â†’ Uses original FAISS order
- Logs warnings but continues processing

---

## ğŸ“ˆ Performance Impact

### **Relevance Improvement:**

- **Before Reranking**: Documents ordered by embedding similarity (may miss semantic nuances)
- **After Reranking**: Documents ordered by query-document relevance (more accurate)

### **Trade-offs:**

- **Accuracy**: âœ… Significantly improved relevance
- **Speed**: âš ï¸ Adds ~0.1-0.5s per query (acceptable for quality gain)
- **Cost**: âœ… Free (uses pre-trained models)

---

## ğŸ¯ Why This Matters for Your Presentation

### **Key Points to Highlight:**

1. **Two-Stage Approach:**

   - Fast initial retrieval (FAISS) + Accurate reranking (Cross-Encoder)
   - Best of both worlds: speed and accuracy

2. **State-of-the-Art Algorithm:**

   - Cross-encoder trained on MS MARCO (industry standard)
   - Specifically designed for information retrieval

3. **Robust Fallback:**

   - BM25 ensures system works even if Cross-Encoder fails
   - Multiple layers of reliability

4. **Measurable Improvement:**
   - Top 4 documents are more relevant than top 4 from initial retrieval
   - Better context â†’ Better LLM responses

---

## ğŸ”¬ Technical Deep Dive

### **Cross-Encoder Architecture:**

```
Input: [CLS] Query [SEP] Document [SEP]
         â†“
    Transformer Encoder
    (6 layers, MiniLM architecture)
         â†“
    Relevance Score (0-1)
```

**Key Features:**

- **Full Attention**: Query tokens can attend to all document tokens
- **Contextual Understanding**: Understands query-document relationships
- **Fine-grained Scoring**: Precise relevance scores

### **BM25 Formula:**

```
BM25(q, d) = Î£ IDF(qi) Ã— (f(qi, d) Ã— (k1 + 1)) / (f(qi, d) + k1 Ã— (1 - b + b Ã— |d|/avgdl))

Where:
- qi = query term
- f(qi, d) = term frequency in document
- IDF(qi) = inverse document frequency
- k1, b = tuning parameters
```

---

## ğŸ“ Summary

**What We Have:**

- âœ… Two-stage retrieval (FAISS + Reranking)
- âœ… Cross-Encoder reranking (primary, state-of-the-art)
- âœ… BM25 reranking (fallback, reliable)
- âœ… Applied to local RAG retrieval
- âœ… Top 4 most relevant documents selected

**How It Improves Relevance:**

1. Initial retrieval gets candidates (fast)
2. Reranking scores each candidate against the query (accurate)
3. Top documents are most relevant to the specific query
4. LLM receives better context â†’ Better answers

**Impact:**

- More relevant documents prioritized
- Better answer quality
- Improved user experience

---

**This is a production-ready reranking implementation that significantly improves retrieval quality!** ğŸ¯
